{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5197aba-5d46-421b-ae3b-4e3034edcfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_core langgraph langchain_openai openpyxl pandas IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa99c304",
   "metadata": {},
   "source": [
    "# First, we need to set the environment variables\n",
    "\n",
    "This script requires OPENAI_API_KEY and LANGSMITH_API_KEY to be set in the environment variables.\n",
    "\n",
    "Also, you need to choose the model to use for the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0f7c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"anki_deck_generator\"\n",
    "\n",
    "gen_data_ai_chat = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "check_data_ai_chat = ChatOpenAI(model=\"gpt-4.1\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05068f09",
   "metadata": {},
   "source": [
    "# Second, we need to define the data structure\n",
    "\n",
    "You need to define the data structure for anki deck by `data_structure` variable:\n",
    "\n",
    "```python\n",
    "data_structure = {\n",
    "    \"field_1_name\": \"field_1_description\",\n",
    "    \"field_name_2\": \"field_2_description\",\n",
    "    \"field_name_3\": \"field_3_description\"\n",
    "}\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "data_structure = {\n",
    "    \"vocabulary\": \"The word to be learned\",\n",
    "    \"meaning_en\": \"The meaning of the word in English\",\n",
    "    \"meaning_vi\": \"The meaning of the word in Japanese\",\n",
    "    \"hiragana\": \"The hiragana of the word\",\n",
    "    \"kanji_vi\": \"Explain the kanji of the word in Vietnamese as they are related to Chinese characters\",\n",
    "    \"sentence\": \"The sentence using the word\",\n",
    "    \"sentence_translation_en\": \"The translation of the sentence in English\",\n",
    "    \"sentence_translation_vi\": \"The translation of the sentence in Vietnamese\",\n",
    "    \"note\": \"Any additional information about the word\"\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "766d9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure = {\n",
    "    \"vocabulary\": \"The word to be learned\",\n",
    "    \"meaning_en\": \"The meaning of the word in English\",\n",
    "    \"meaning_vi\": \"The meaning of the word in Japanese\",\n",
    "    \"hiragana\": \"The hiragana of the word\",\n",
    "    \"kanji_vi\": \"Explain the kanji of the word in Vietnamese as they are related to Chinese characters\",\n",
    "    \"sentence\": \"The sentence using the word\",\n",
    "    \"sentence_translation_en\": \"The translation of the sentence in English\",\n",
    "    \"sentence_translation_vi\": \"The translation of the sentence in Vietnamese\",\n",
    "    \"note\": \"Any additional information about the word\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a84212",
   "metadata": {},
   "source": [
    "# Third, we need to list out all the words to be learned, with note for the AI if any\n",
    "\n",
    "You can/should generate this list using AI.\n",
    "\n",
    "```python\n",
    "vocabularies = [\"word_without_note_1\", \"word_without_note_2\", \"word_without_note_3\"]\n",
    "vocabularies_with_notes = {\n",
    "    \"word_with_note_1\": \"note_to_ai_for_word_with_note_1\",\n",
    "    \"word_with_note_2\": \"note_to_ai_for_word_with_note_2\",\n",
    "    \"word_with_note_3\": \"note_to_ai_for_word_with_note_3\"\n",
    "}\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "vocabularies = [\"猫\", \"電車\", \"公園\"] \n",
    "vocabularies_with_notes = {\n",
    "    \"勉強\": \"I am truggle to remember it, so please explain it more in detail with more examples\",\n",
    "    \"学校\": \"This word is quite easy, so no need to explain it in detail\",\n",
    "    \"食べ物\": \"\", # Empty note is also fine\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b4c95c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "vocabularies: List[str] = [\"猫\", \"電車\", \"公園\"] \n",
    "vocabularies_with_notes: Dict[str, str] = {\n",
    "    \"勉強\": \"I am truggle to remember it, so please explain it more in detail with more examples\",\n",
    "    \"学校\": \"This word is quite easy, so no need to explain it in detail\",\n",
    "    \"食べ物\": \"\", # Empty note is also fine\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b77618",
   "metadata": {},
   "source": [
    "# Finally, let's verify the data you added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69ca726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data verification passed!\n"
     ]
    }
   ],
   "source": [
    "def verify_vocabularies_data(vocabularies: List[str], vocabularies_with_notes: Dict[str, str]) -> bool:\n",
    "    \"\"\"\n",
    "    Verify that vocabularies and vocabularies_with_notes follow the expected types and structure.\n",
    "    \n",
    "    Args:\n",
    "        vocabularies: List of vocabulary words\n",
    "        vocabularies_with_notes: Dictionary mapping vocabulary words to notes\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if data is valid, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if vocabularies is a list\n",
    "        if not isinstance(vocabularies, list):\n",
    "            print(\"Error: vocabularies must be a list\")\n",
    "            return False\n",
    "            \n",
    "        # Check if vocabularies_with_notes is a dictionary\n",
    "        if not isinstance(vocabularies_with_notes, dict):\n",
    "            print(\"Error: vocabularies_with_notes must be a dictionary\")\n",
    "            return False\n",
    "            \n",
    "        # Check if all items in vocabularies are strings\n",
    "        for i, word in enumerate(vocabularies):\n",
    "            if not isinstance(word, str):\n",
    "                print(f\"Error: vocabularies[{i}] must be a string, got {type(word)}\")\n",
    "                return False\n",
    "                \n",
    "        # Check if all keys and values in vocabularies_with_notes are strings\n",
    "        for word, note in vocabularies_with_notes.items():\n",
    "            if not isinstance(word, str):\n",
    "                print(f\"Error: Key in vocabularies_with_notes must be a string, got {type(word)}\")\n",
    "                return False\n",
    "            if not isinstance(note, str):\n",
    "                print(f\"Error: Value in vocabularies_with_notes must be a string, got {type(note)}\")\n",
    "                return False\n",
    "                \n",
    "        print(\"✓ Data verification passed!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during verification: {e}\")\n",
    "        return False\n",
    "\n",
    "assert verify_vocabularies_data(vocabularies, vocabularies_with_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30f759",
   "metadata": {},
   "source": [
    "# All is good, now we can start to generate the anki deck\n",
    "\n",
    "You can find the csv under `./output/anki_deck_data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ea2f146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabularies after merging: 6\n",
      "All vocabularies with notes:\n",
      "  勉強: I am truggle to remember it, so please explain it more in detail with more examples\n",
      "  学校: This word is quite easy, so no need to explain it in detail\n",
      "  食べ物: (no note)\n",
      "  猫: (no note)\n",
      "  電車: (no note)\n",
      "  公園: (no note)\n",
      "\n",
      "✓ Vocabularies merged successfully!\n"
     ]
    }
   ],
   "source": [
    "# Merge vocabularies and vocabularies_with_notes into a single dictionary\n",
    "for word in vocabularies:\n",
    "    if word not in vocabularies_with_notes:\n",
    "        vocabularies_with_notes[word] = \"\"\n",
    "\n",
    "print(f\"Total vocabularies after merging: {len(vocabularies_with_notes)}\")\n",
    "print(\"All vocabularies with notes:\")\n",
    "for word, note in vocabularies_with_notes.items():\n",
    "    if note:\n",
    "        print(f\"  {word}: {note}\")\n",
    "    else:\n",
    "        print(f\"  {word}: (no note)\")\n",
    "        \n",
    "print(\"\\n✓ Vocabularies merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4a8d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data structure for the AI LangGraph\n",
    "\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "class AnkiDeckData(TypedDict):\n",
    "    vocabularies_with_notes: Dict[str, str] # The vocabularies with notes\n",
    "    ai_gen_data: List[Dict[str, str]] # The data generated by the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5316750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import json\n",
    "\n",
    "def node_generate_anki_deck_data(deck_data: AnkiDeckData) -> AnkiDeckData:\n",
    "    \"\"\"\n",
    "    Generate Anki deck data for each vocabulary word using AI.\n",
    "    \"\"\"\n",
    "    vocabularies_with_notes = deck_data[\"vocabularies_with_notes\"]\n",
    "    ai_gen_data = []\n",
    "    \n",
    "    print(f\"Generating data for {len(vocabularies_with_notes)} vocabulary words...\")\n",
    "    \n",
    "    for i, (vocab_word, note) in enumerate(vocabularies_with_notes.items(), 1):\n",
    "        print(f\"Processing {i}/{len(vocabularies_with_notes)}: {vocab_word}\")\n",
    "        \n",
    "        # Create the prompt for AI\n",
    "        prompt = f\"\"\"\n",
    "You are a Japanese language expert helping to create Anki flashcard data. \n",
    "Generate comprehensive information for the Japanese word: {vocab_word}\n",
    "\n",
    "Please provide the following information in JSON format with these exact keys:\n",
    "- vocabulary: The original Japanese word\n",
    "- meaning_en: The meaning of the word in English\n",
    "- meaning_vi: The meaning of the word in Vietnamese (not Japanese!)\n",
    "- hiragana: The hiragana reading of the word (if applicable, empty string if not)\n",
    "- kanji_vi: Explain the kanji components in Vietnamese, relating them to Chinese characters and their meanings\n",
    "- sentence: A natural Japanese sentence using this word\n",
    "- sentence_translation_en: English translation of the sentence\n",
    "- sentence_translation_vi: Vietnamese translation of the sentence\n",
    "- note: Additional helpful information about the word (usage notes, formality level, etc.)\n",
    "\n",
    "Special instructions for this word: {note if note else \"No special instructions\"}\n",
    "\n",
    "Please respond with valid JSON only, no additional text.\n",
    "Example format:\n",
    "{{\n",
    "    \"vocabulary\": \"猫\",\n",
    "    \"meaning_en\": \"cat\",\n",
    "    \"meaning_vi\": \"con mèo\",\n",
    "    \"hiragana\": \"ねこ\",\n",
    "    \"kanji_vi\": \"猫: Chữ Hán này có nghĩa là 'mèo', từ gốc Trung Quốc 貓\",\n",
    "    \"sentence\": \"私は猫が好きです。\",\n",
    "    \"sentence_translation_en\": \"I like cats.\",\n",
    "    \"sentence_translation_vi\": \"Tôi thích mèo.\",\n",
    "    \"note\": \"Common word for cat, used in everyday conversation\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Get AI response\n",
    "            response = gen_data_ai_chat.invoke(prompt)\n",
    "            response_text = response.content.strip()\n",
    "            \n",
    "            # Try to parse JSON response\n",
    "            try:\n",
    "                word_data = json.loads(response_text)\n",
    "                \n",
    "                # Validate that all required fields are present\n",
    "                required_fields = list(data_structure.keys())\n",
    "                missing_fields = [field for field in required_fields if field not in word_data]\n",
    "                \n",
    "                if missing_fields:\n",
    "                    print(f\"Warning: Missing fields for {vocab_word}: {missing_fields}\")\n",
    "                    # Fill missing fields with empty strings\n",
    "                    for field in missing_fields:\n",
    "                        word_data[field] = \"\"\n",
    "                \n",
    "                # Ensure vocabulary field matches the input word\n",
    "                word_data[\"vocabulary\"] = vocab_word\n",
    "                \n",
    "                ai_gen_data.append(word_data)\n",
    "                print(f\"✓ Successfully generated data for {vocab_word}\")\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON for {vocab_word}: {e}\")\n",
    "                print(f\"Raw response: {response_text[:200]}...\")\n",
    "                \n",
    "                # Create fallback data\n",
    "                fallback_data = {field: \"\" for field in data_structure.keys()}\n",
    "                fallback_data[\"vocabulary\"] = vocab_word\n",
    "                fallback_data[\"note\"] = f\"Error generating data: {str(e)}\"\n",
    "                ai_gen_data.append(fallback_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating data for {vocab_word}: {e}\")\n",
    "            \n",
    "            # Create fallback data\n",
    "            fallback_data = {field: \"\" for field in data_structure.keys()}\n",
    "            fallback_data[\"vocabulary\"] = vocab_word\n",
    "            fallback_data[\"note\"] = f\"Error generating data: {str(e)}\"\n",
    "            ai_gen_data.append(fallback_data)\n",
    "    \n",
    "    print(f\"\\n✓ Generated data for {len(ai_gen_data)} vocabulary words\")\n",
    "    \n",
    "    # Update the deck data\n",
    "    deck_data[\"ai_gen_data\"] = ai_gen_data\n",
    "    return deck_data\n",
    "\n",
    "def node_check_anki_deck_data(deck_data: AnkiDeckData) -> AnkiDeckData:\n",
    "    \"\"\"\n",
    "    Check and validate the generated Anki deck data.\n",
    "    This function will review the generated data for quality and completeness.\n",
    "    \"\"\"\n",
    "    ai_gen_data = deck_data[\"ai_gen_data\"]\n",
    "    \n",
    "    print(f\"Checking data quality for {len(ai_gen_data)} vocabulary words...\")\n",
    "    \n",
    "    for i, word_data in enumerate(ai_gen_data):\n",
    "        vocab_word = word_data.get(\"vocabulary\", \"Unknown\")\n",
    "        print(f\"Checking {i+1}/{len(ai_gen_data)}: {vocab_word}\")\n",
    "        \n",
    "        # Check for missing or empty critical fields\n",
    "        critical_fields = [\"vocabulary\", \"meaning_en\", \"meaning_vi\", \"hiragana\"]\n",
    "        missing_critical = []\n",
    "        \n",
    "        for field in critical_fields:\n",
    "            if not word_data.get(field, \"\").strip():\n",
    "                missing_critical.append(field)\n",
    "        \n",
    "        if missing_critical:\n",
    "            print(f\"  ⚠️  Missing critical fields: {missing_critical}\")\n",
    "            \n",
    "            # Try to regenerate missing data using AI\n",
    "            if len(missing_critical) > 1:  # If multiple critical fields are missing\n",
    "                prompt = f\"\"\"\n",
    "Please provide missing information for the Japanese word: {vocab_word}\n",
    "\n",
    "I need the following information in JSON format:\n",
    "{chr(10).join([f\"- {field}: {data_structure[field]}\" for field in missing_critical])}\n",
    "\n",
    "Respond with JSON containing only the missing fields:\n",
    "\"\"\"\n",
    "                try:\n",
    "                    response = check_data_ai_chat.invoke(prompt)\n",
    "                    missing_data = json.loads(response.content.strip())\n",
    "                    \n",
    "                    # Update the missing fields\n",
    "                    for field in missing_critical:\n",
    "                        if field in missing_data and missing_data[field].strip():\n",
    "                            word_data[field] = missing_data[field]\n",
    "                            print(f\"  ✓ Updated {field}\")\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"  ❌ Could not regenerate missing data: {e}\")\n",
    "        \n",
    "        # Validate Japanese characters\n",
    "        if word_data.get(\"vocabulary\") and not any('\\u3040' <= c <= '\\u309F' or '\\u30A0' <= c <= '\\u30FF' or '\\u4E00' <= c <= '\\u9FAF' for c in word_data[\"vocabulary\"]):\n",
    "            print(f\"  ⚠️  Warning: '{vocab_word}' doesn't contain Japanese characters\")\n",
    "        \n",
    "        # Check if hiragana is appropriate\n",
    "        hiragana = word_data.get(\"hiragana\", \"\")\n",
    "        if hiragana and not all('\\u3040' <= c <= '\\u309F' or c in 'ー' for c in hiragana if c.strip()):\n",
    "            print(f\"  ⚠️  Warning: Hiragana field contains non-hiragana characters: {hiragana}\")\n",
    "        \n",
    "        print(f\"  ✓ Checked {vocab_word}\")\n",
    "    \n",
    "    print(f\"\\n✓ Data quality check completed for {len(ai_gen_data)} words\")\n",
    "    \n",
    "    return deck_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the data generation function\n",
    "# Create initial deck data\n",
    "initial_deck_data: AnkiDeckData = {\n",
    "    \"vocabularies_with_notes\": vocabularies_with_notes,\n",
    "    \"ai_gen_data\": []\n",
    "}\n",
    "\n",
    "# Generate the data\n",
    "generated_deck_data = node_generate_anki_deck_data(initial_deck_data)\n",
    "\n",
    "# Display a sample of the generated data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE GENERATED DATA:\")\n",
    "print(\"=\"*50)\n",
    "if generated_deck_data[\"ai_gen_data\"]:\n",
    "    sample_word = generated_deck_data[\"ai_gen_data\"][0]\n",
    "    for field, value in sample_word.items():\n",
    "        print(f\"{field}: {value}\")\n",
    "        \n",
    "print(f\"\\nTotal words processed: {len(generated_deck_data['ai_gen_data'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the generated data to CSV for Anki import\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "\n",
    "# Convert the generated data to a pandas DataFrame\n",
    "if generated_deck_data[\"ai_gen_data\"]:\n",
    "    df = pd.DataFrame(generated_deck_data[\"ai_gen_data\"])\n",
    "    \n",
    "    # Reorder columns according to data_structure order\n",
    "    ordered_columns = list(data_structure.keys())\n",
    "    df = df.reindex(columns=ordered_columns)\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_path = \"./output/anki_deck_data.csv\"\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"✓ Anki deck data exported to: {csv_path}\")\n",
    "    print(f\"✓ Total words: {len(df)}\")\n",
    "    print(f\"✓ Fields: {', '.join(df.columns)}\")\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nData Summary:\")\n",
    "    for col in df.columns:\n",
    "        non_empty = df[col].astype(str).str.strip().ne('').sum()\n",
    "        print(f\"  {col}: {non_empty}/{len(df)} entries filled ({non_empty/len(df)*100:.1f}%)\")\n",
    "        \n",
    "    # Show first few rows\n",
    "    print(f\"\\nFirst 3 rows preview:\")\n",
    "    print(df.head(3).to_string())\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No data to export. Please run the generation function first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
